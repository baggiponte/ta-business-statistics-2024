---
title: "Clustering - Hierarchical"
output: html_notebook
editor_options:
  chunk_output_type: console
---

# Goal of this lecture

This lecture covers hierarchical clustering, as well as the last lecture about clustering. We shall cover:

1. Hierarchical clustering and distance matrices
3. Comparing different linkage methods
4. Using the `factoextra` and `NbClust` package to determine the optimal number of clusters

Finally, we shall wrap up the main takeaways from clustering.

# Libraries

```{r setup echo=FALSE, warning=FALSE}
library(tidyverse)
library(palmerpenguins)

library(factoextra)
library(NbClust)

theme_set(theme_minimal())

set.seed(42)
```

# Data preparation

The same as the previous lecture.

```{r}
data <- penguins %>%
  drop_na() %>%
  select(where(is.numeric), -year) %>%
  # not needed, can simply write `%>% scale()`
  mutate(across(everything(), ~scale(.x) %>% as.vector()))

data %>% head()
```

# Computing distances

With base R function, we need to compute first a distance matrix:

```{r}
distance <- data %>% dist()
```

## Other distance measures

There are plenty of [distance measures](https://www.datanovia.com/en/lessons/clustering-distance-measures/) to use for clustering. The built-in `dist` method can compute "euclidean", "maximum", "manhattan", "canberra", "binary" or "minkowski" distances. The `get_dist()` method in `factoextra` also offers the correlation-based measures "pearson", "spearman" or "kendall".

You can visualise distances with `factoextra::fviz_dist` - though it becomes quite hard to read if the number of points exceeds a relatively small threshold.

```{r}
fviz_dist(distance)
```

# Fitting the algorithm

Then we can use this distance matrix to fit the hierarchical clustering models:

```{r}
distance %>%
  hclust(method="single")
```

We can visualise this result as a dendrogram with the built-in `plot` method:

```{r}
distance %>%
  hclust(method="single") %>%
  plot()
```

Though, as per with the distance plot, it quickly becomes hard to read the more points there are. The `cutree` method in base R can be applied to the result of hierarchical clustering to return the labels of the groups each data point is assigned to.

```{r}
distance %>%
  hclust(method="single") %>%
  cutree(k=4) # or we can specify the height `h`
```

With `factoextra`, we can `cut` the dendrogram at a certain height. This cut highlights all points that end up in the same cluster, providing a quick way to inspect their number.

```{r}
hclust_single <- distance %>%
  hclust(method="single")

hclust_single %>%
  fviz_dend(k=6)
```

# Different linkages

```{r}
hclust_complete <- distance %>%
  hclust(method="complete")

hclust_complete %>%
  fviz_dend(k=6)
```

```{r}
hclust_avg <- distance %>%
  hclust(method="average")

hclust_avg %>%
  fviz_dend(k=6)
```

```{r}
hclust_centroid <- distance %>%
  hclust(method="centroid")

hclust_centroid %>%
  fviz_dend(k=6)
```

# Determining the optimal number of clusters

```{r}
nbclust_single <- data %>%
  NbClust(
    distance="euclidean",
    min.nc = 2,
    max.nc = 10,
    method = "single",
    index="all"
    )

# fviz_nbclust(nbclust_single) # should work, actually errors!
```

```{r}
nbclust_comp <- data %>%
  NbClust(
    distance="euclidean",
    min.nc = 2,
    max.nc = 10,
    method = "complete",
    index="all"
    )
```

```{r}
nbclust_avg <- data %>%
  NbClust(
    distance="euclidean",
    min.nc = 2,
    max.nc = 10,
    method = "average",
    index="all"
    )
```

```{r}
nbclust_centroids <- data %>%
  NbClust(
    distance="euclidean",
    min.nc = 2,
    max.nc = 10,
    method = "centroid",
    index="all"
  )
```

We can also run `NbClust` with the `kmeans` algorithm:

```{r}
nbclust_kmeans <- data %>%
  NbClust(
    distance="euclidean",
    min.nc = 2,
    max.nc = 10,
    method = "kmeans",
    index="all"
  )
```

# Clustering takeaways

Clustering is an unsupervised learning task, meaning that we do not have a target variable to predict. We saw two types of clustering: hierarchical and non-hierarchical. Regardless of the type, clustering algorithms are distance-based: in other words, they need to compute the distance between all points in the dataset. This makes them computationally intensive, but also (and most importantly) **very sensitive to the scale of the data**. **Make sure to standardise your data** before fitting the model.

Since it's an unsupervised problem, there isn't one true answer and, in the end, we are the ones to make the choice of the final number of clusters. Nevertheless, we have several tools at our disposal to guide our decision:

1. Intuitions from the data, derived from their exploration (also known as *exploratory data analysis* or *EDA*).
2. Several error metrics.
3. Automatic procedures to fit and display the results, given a range of clusters.

The error metrics are usually interpretable as a measurement of the "goodness" of the clusters:

1. Inertia, or within-cluster sum of squares (WSS), measures the "compactness" of the cluster.
2. Between-cluster sum of squares (BSS) measures how far apart clusters are.
3. The Calinski-Harabasz score (CH) is the ratio of WSS to BSS.
4. The Silhouette score uses the distance of a sample to the cluster it belongs to and the nearest one that the sample is not a part of.
5. The gap statistic attempts to identify the point at which the clustering structure becomes meaningful.

The literature proposes some heuristics to assess the number of clusters, such as the so-called "Elbow criterion" (which is [not always reliable](https://en.wikipedia.org/wiki/Elbow_method_(clustering)#Criticism)).

For hierarchical clustering, we also need to specify the linkage method - each one with its pros and cons.

Algorithms like dbscan and hdbscan do not require to specify the number of clusters in advance, but we still need to choose an `epsilon` parameter carefully.
