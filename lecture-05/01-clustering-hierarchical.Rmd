---
title: "Hierarchical Clustering - Agglomerative and Divisive"
output: html_notebook
editor_options:
  chunk_output_type: console
---

# Goal of this lecture

This lecture covers hierarchical clustering. There are three key ingredients to hierarchical clustering:

1. gglomerative vs divisive. 
2. Distance metrics.
3. Linkage methods

We'll use the `factoextra`, `cluster` and `NbClust` package to help us determine the optimal number of clusters. Finally, we shall wrap up the main takeaways from clustering.

# Libraries

```{r setup echo=FALSE, warning=FALSE}
library(tidyverse)
library(palmerpenguins)

library(factoextra)
library(NbClust)
library(cluster)

theme_set(theme_minimal())

set.seed(42)
```

# Data preparation

The same as the previous lecture.

```{r}
data <- penguins %>%
  drop_na() %>%
  select(where(is.numeric), -year) %>%
  scale() %>%
  as_tibble()

data %>% head()
```

# Agglomerative and divisive clustering

Hierarchical clustering can be divided into two main types:

* Agglomerative clustering: Commonly referred to as AGNES (AGglomerative NESting) works in a bottom-up manner. That is, each observation is initially considered as a single-element cluster (leaf). At each step of the algorithm, the two clusters that are the most similar are combined into a new bigger cluster (nodes). This procedure is iterated until all points are a member of just one single big cluster (root).
* Divisive hierarchical clustering: Commonly referred to as DIANA (DIvise ANAlysis) works in a top-down manner. DIANA is like the reverse of AGNES. It begins with the root, in which all observations are included in a single cluster. At each step of the algorithm, the current cluster is split into two clusters that are considered most heterogeneous. The process is iterated until all observations are in their own cluster.

![](https://bradleyboehmke.github.io/HOML/images/dendrogram2.png)

Agglomerative clustering is good at identifying small clusters. Divisive hierarchical clustering, on the other hand, is better at identifying large clusters.

# Computing distances

With base R function, we need to compute first a distance matrix:

```{r}
distance <- data %>% dist()
```

## Other distance measures

There are plenty of [distance measures](https://www.datanovia.com/en/lessons/clustering-distance-measures/) to use for clustering. The built-in `dist` method can compute "euclidean", "maximum", "manhattan", "canberra", "binary" or "minkowski" distances. The `get_dist()` method in `factoextra` also offers the correlation-based measures "pearson", "spearman" or "kendall".

You can visualise distances with `factoextra::fviz_dist` - though it becomes quite hard to read if the number of points exceeds a relatively small threshold.

```{r}
fviz_dist(distance)
```

# Divisive clustering with `diana`

```{r}
data |>
  cluster::diana() |>
  fviz_dend()
```

# Agglomerative clustering: linkages

This time, the hyperparameter is the linkage (*legame*). The most common linkages are:

1. **Single Linkage (Minimum Linkage)**:
   - **Definition**: The distance between two clusters is the shortest distance between any two points in the two clusters.
   - **Intuition**: Imagine drawing a line between the two closest points from each cluster. Single linkage tends to find the narrowest or most direct connection between clusters, leading to elongated, chain-like clusters. It's like linking the nearest neighbors.

2. **Complete Linkage (Maximum Linkage)**:
   - **Definition**: The distance between two clusters is the longest distance between any two points in the two clusters.
   - **Intuition**: Imagine finding the farthest pair of points between the clusters and measuring that distance. Complete linkage ensures that all points in a cluster are close to all points in the other cluster, creating more compact and spherical clusters. It's like ensuring the maximum separation between groups is minimized.

3. **Average Linkage (Mean Linkage)**:
   - **Definition**: The distance between two clusters is the average distance between all pairs of points in the two clusters.
   - **Intuition**: Calculate the mean of all pairwise distances between points in the two clusters. This method balances the extremes of single and complete linkage, often resulting in moderately compact clusters. It's like considering the overall average distance to decide if clusters should merge.

4. **Centroid Linkage**:
   - **Definition**: The distance between two clusters is the distance between their centroids (mean positions of all points in the clusters).
   - **Intuition**: Think of each cluster as a single point at its center. The clusters are merged based on the distance between these center points. This method can sometimes create non-intuitive cluster shapes if the centroids move significantly after merging.

5. **Ward's Linkage**:
   - **Definition**: The distance between two clusters is the increase in the total within-cluster variance when they are merged.
   - **Intuition**: Ward's method aims to keep clusters as compact and similar in size as possible. It looks for the merger that causes the least increase in the overall variance within the clusters. It's like finding the most homogeneous combination, minimizing the "spread" of the combined cluster.
   
We need to choose the linkage measure appropriately, as different linkages will produce different cluster memberships on the same data.

## Single linkage

```{r}
distance %>%
  hclust(method = "single")
```

We can visualise this result as a dendrogram with the built-in `plot` method:

```{r}
distance %>%
  hclust(method = "single") %>%
  plot()
```

Though, as per with the distance plot, it quickly becomes hard to read the more points there are. The `cutree` method in base R can be applied to the result of hierarchical clustering to return the labels of the groups each data point is assigned to.

```{r}
distance %>%
  hclust(method = "single") %>%
  cutree(k = 4) # or we can specify the height `h`
```

With `factoextra`, we can `cut` the dendrogram at a certain height. This cut highlights all points that end up in the same cluster, providing a quick way to inspect their number.

```{r}
hclust_single <- distance %>%
  hclust(method = "single")

hclust_single %>%
  fviz_dend(k = 6)
```

# Different linkages

```{r}
hclust_complete <- distance %>%
  hclust(method = "complete")

hclust_complete %>%
  fviz_dend(k = 6)
```

```{r}
hclust_avg <- distance %>%
  hclust(method = "average")

hclust_avg %>%
  fviz_dend(k = 6)
```

```{r}
hclust_centroid <- distance %>%
  hclust(method = "centroid")

hclust_centroid %>%
  fviz_dend(k = 6)
```

# Determining the optimal number of clusters

```{r}
nbclust_single <- data %>%
  NbClust(
    distance = "euclidean",
    min.nc = 2,
    max.nc = 10,
    method = "single",
    index = "all"
  )

# fviz_nbclust(nbclust_single) # should work, actually errors!
```

```{r}
nbclust_comp <- data %>%
  NbClust(
    distance = "euclidean",
    min.nc = 2,
    max.nc = 10,
    method = "complete",
    index = "all"
  )
```

```{r}
nbclust_avg <- data %>%
  NbClust(
    distance = "euclidean",
    min.nc = 2,
    max.nc = 10,
    method = "average",
    index = "all"
  )
```

```{r}
nbclust_centroids <- data %>%
  NbClust(
    distance = "euclidean",
    min.nc = 2,
    max.nc = 10,
    method = "centroid",
    index = "all"
  )
```

We can also run `NbClust` with the `kmeans` algorithm:

```{r}
nbclust_kmeans <- data %>%
  NbClust(
    distance = "euclidean",
    min.nc = 2,
    max.nc = 10,
    method = "kmeans",
    index = "all"
  )
```

# Clustering takeaways

Clustering is an unsupervised learning task, meaning that we do not have a target variable to predict. We saw two types of clustering: hierarchical and non-hierarchical. Regardless of the type, clustering algorithms are distance-based: in other words, they need to compute the distance between all points in the dataset. This makes them computationally intensive, but also (and most importantly) **very sensitive to the scale of the data**. **Make sure to standardise your data** before fitting the model.

Since it's an unsupervised problem, there isn't one true answer and, in the end, we are the ones to make the choice of the final number of clusters. Nevertheless, we have several tools at our disposal to guide our decision:

1. Intuitions from the data, derived from their exploration (also known as *exploratory data analysis* or *EDA*).
2. Several error metrics.
3. Automatic procedures to fit and display the results, given a range of clusters.

The error metrics are usually interpretable as a measurement of the "goodness" of the clusters:

1. Inertia, or within-cluster sum of squares (WSS), measures the "compactness" of the cluster.
2. Between-cluster sum of squares (BSS) measures how far apart clusters are.
3. The Calinski-Harabasz score (CH) is the ratio of WSS to BSS.
4. The Silhouette score uses the distance of a sample to the cluster it belongs to and the nearest one that the sample is not a part of.
5. The gap statistic attempts to identify the point at which the clustering structure becomes meaningful.

The literature proposes some heuristics to assess the number of clusters, such as the so-called "Elbow criterion" (which is [not always reliable](https://en.wikipedia.org/wiki/Elbow_method_(clustering)#Criticism)).

For hierarchical clustering, we also need to specify the linkage method - each one with its pros and cons.

Algorithms like dbscan and hdbscan do not require to specify the number of clusters in advance, but we still need to choose an `epsilon` parameter carefully.

## References

* https://bradleyboehmke.github.io/HOML/hierarchical.html
